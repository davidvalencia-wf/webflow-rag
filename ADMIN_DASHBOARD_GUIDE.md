# Admin Dashboard Guide - Webflow RAG Analytics Platform

**Version**: 1.5.0
**Last Updated**: 2025-11-19
**Status**: Production-Ready

---

## Table of Contents

1. [Overview](#overview)
2. [Getting Started](#getting-started)
3. [Dashboard Tabs](#dashboard-tabs)
4. [Key Metrics Explained](#key-metrics-explained)
5. [API Endpoints](#api-endpoints)
6. [Component Architecture](#component-architecture)
7. [Data Export](#data-export)
8. [Best Practices](#best-practices)
9. [Troubleshooting](#troubleshooting)
10. [Future Enhancements](#future-enhancements)

---

## Overview

The Admin Dashboard is a comprehensive analytics and monitoring platform for the Webflow RAG system. It provides real-time insights into:

- **Content Gaps** - Identify missing or inadequate documentation
- **Usage Metrics** - Track user engagement and query volume
- **Performance** - Monitor system latency and error rates
- **Quality** - Measure answer quality and user satisfaction
- **Cost** - Track API usage and spending
- **Content** - Analyze query patterns and source citations
- **Trends** - Visualize temporal patterns and emerging topics
- **Topic Trends** - Advanced topic trend detection
- **Sentiment** - Understand user feedback and complaints

### Key Features

- âœ… **9 Specialized Dashboards** - Each focused on specific analytics needs
- âœ… **Real-time Data** - Live metrics updated from production queries
- âœ… **Interactive Charts** - Powered by Recharts with responsive design
- âœ… **CSV Export** - Download data for external analysis
- âœ… **Mobile Responsive** - Full functionality on all screen sizes
- âœ… **Type-Safe** - 100% TypeScript with Zod validation
- âœ… **Performance Optimized** - Lazy loading and efficient rendering

### Access

**URL**: `/admin`
**Authentication**: Not yet implemented (coming in v1.6.0)
**Browsers**: Chrome 90+, Firefox 88+, Safari 14+, Edge 90+

---

## Getting Started

### Prerequisites

1. **Development Environment**:
   ```bash
   cd apps/web
   pnpm install
   pnpm dev
   ```

2. **Access Dashboard**:
   - Navigate to `http://localhost:3000/admin`
   - Or in production: `https://your-domain.webflow.io/admin`

3. **Data Requirements**:
   - D1 database with queries, responses, and feedback tables populated
   - At least 10+ queries in the database for meaningful analytics
   - Admin API routes must be functional

### Navigation

The dashboard uses a **collapsible sidebar** navigation:

- **Desktop**: Full sidebar with icons and labels
- **Tablet**: Collapsed sidebar with icons only (hover to expand)
- **Mobile**: Hidden sidebar (tap hamburger menu to toggle)

### Dashboard Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Sidebar  â”‚  Main Content Area          â”‚
â”‚  (Tabs)   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚           â”‚  â”‚  Tab Content        â”‚   â”‚
â”‚  Content  â”‚  â”‚  - Metrics          â”‚   â”‚
â”‚  Overview â”‚  â”‚  - Charts           â”‚   â”‚
â”‚  ...      â”‚  â”‚  - Tables           â”‚   â”‚
â”‚           â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Dashboard Tabs

### 1. Content Gaps ğŸ¯

**Purpose**: Identify areas where the knowledge base is lacking or incomplete.

**Key Metrics**:
- **Unanswered Questions** (confidence < 0.4)
- **Zero Citation Queries** (no sources found)
- **Most Regenerated Answers** (users unhappy with initial response)
- **Topic Clusters** (related unanswered queries)
- **Trending Gaps** (emerging problem areas)

**Use Cases**:
- Prioritize new documentation to create
- Identify common user pain points
- Guide content strategy for knowledge base expansion
- Detect emerging topics not yet covered

**Data Table Columns**:
| Column | Description |
|--------|-------------|
| Query | The question users asked |
| Count | Number of times asked |
| Avg Confidence | Average match quality (0-1) |
| Last Asked | Most recent occurrence |
| Priority | Auto-calculated urgency (High/Medium/Low) |

**Actions**:
- **Create Documentation** - Add missing content to knowledge base
- **Improve Sources** - Enhance existing documentation clarity
- **Export to CSV** - Share gaps with content team

---

### 2. Overview ğŸ“Š

**Purpose**: High-level usage analytics and user engagement metrics.

**Key Metrics**:
- **Total Queries** (all-time, today, 7d, 30d)
- **Active Users** (DAU, WAU, MAU)
- **Avg Queries per User**
- **Avg Session Duration** (minutes)
- **Retention Curve** (user return rates)

**Charts**:
1. **Query Volume Over Time** - Line chart showing daily query counts
2. **Active Users Trend** - DAU/WAU/MAU comparison
3. **Retention Cohorts** - User retention by signup week

**Use Cases**:
- Track product adoption and growth
- Measure user engagement
- Identify usage patterns (peak hours, days)
- Calculate churn and retention

**Formulas**:
```typescript
DAU = Unique users who queried today
WAU = Unique users who queried in last 7 days
MAU = Unique users who queried in last 30 days

Retention Rate (D7) = Users who returned after 7 days / Total new users
```

---

### 3. Performance âš¡

**Purpose**: Monitor system latency, error rates, and infrastructure health.

**Key Metrics**:
- **Response Latency** (P50, P95, P99 in milliseconds)
- **Error Rate** (% of failed requests)
- **Cache Hit Rate** (% of queries served from cache)
- **Streaming Failure Rate** (% of incomplete responses)
- **Latency Breakdown** (Pinecone vs OpenAI time)

**Charts**:
1. **Latency Percentiles** - Line chart showing P50/P95/P99 over time
2. **Error Rate Trend** - Area chart of errors by type
3. **Cache Performance** - Bar chart of hit/miss rates

**Use Cases**:
- Detect performance regressions
- Optimize slow queries
- Monitor infrastructure health
- Plan capacity and scaling

**Performance Targets**:
| Metric | Target | Alert Threshold |
|--------|--------|-----------------|
| P50 Latency | < 1500ms | > 2000ms |
| P95 Latency | < 3000ms | > 4000ms |
| Error Rate | < 2% | > 5% |
| Cache Hit Rate | > 40% | < 30% |

---

### 4. Quality ğŸŒŸ

**Purpose**: Measure answer quality and user satisfaction.

**Key Metrics**:
- **Avg Confidence Score** (0-1, based on vector similarity)
- **Feedback Ratio** (thumbs up / thumbs down)
- **Regeneration Rate** (% of answers regenerated)
- **Top "Not Helpful" Reasons** (free-text feedback analysis)

**Charts**:
1. **Confidence Distribution** - Histogram of answer confidence scores
2. **Feedback Trend** - Line chart of satisfaction over time
3. **Regeneration Reasons** - Pie chart of why users regenerate

**Use Cases**:
- Track answer quality improvements
- Identify common dissatisfaction patterns
- Measure impact of model/prompt changes
- Prioritize quality improvements

**Quality Score Calculation**:
```typescript
Quality Score = (
  Avg Confidence * 0.4 +
  Feedback Ratio * 0.3 +
  (1 - Regeneration Rate) * 0.3
)

Ranges:
- High Quality: > 0.7
- Medium Quality: 0.5 - 0.7
- Low Quality: < 0.5
```

---

### 5. Cost ğŸ’°

**Purpose**: Track API usage costs and optimize spending.

**Key Metrics**:
- **OpenAI Costs**:
  - Embeddings (text-embedding-3-small)
  - Completions (gpt-4o-mini)
- **Pinecone Costs** (usually $0 on free tier)
- **Cloudflare Costs**:
  - KV reads/writes
  - D1 queries
  - R2 storage
- **Cost per Query** (total cost / total queries)
- **Monthly Burn Rate** (projected monthly cost)

**Charts**:
1. **Cost Breakdown** - Pie chart of spending by service
2. **Daily Cost Trend** - Line chart showing daily spend
3. **Cost per Query** - Bar chart showing cost efficiency over time

**Use Cases**:
- Monitor API spending
- Identify cost optimization opportunities
- Budget forecasting
- ROI analysis

**Cost Optimization Tips**:
1. **Increase Cache Hit Rate** - Saves ~50% on embedding costs
2. **Reduce top_k** - Fewer chunks = lower completion costs
3. **Use gpt-4o-mini** - 10x cheaper than gpt-4o
4. **Batch operations** - Reduce D1/KV read costs

**Pricing Reference** (as of Nov 2025):
| Service | Cost | Unit |
|---------|------|------|
| OpenAI Embeddings | $0.02 | per 1M tokens |
| OpenAI GPT-4o-mini | $0.15 | per 1M input tokens |
| Pinecone Free Tier | $0.00 | up to 100k vectors |
| Cloudflare KV | $0.50 | per 1M reads |
| Cloudflare D1 | Free | first 5M rows read |

---

### 6. Content ğŸ“š

**Purpose**: Analyze query patterns and source citation distribution.

**Key Metrics**:
- **Top Queries** (most frequently asked)
- **Top Cited Sources** (most referenced documentation)
- **Uncited Sources** (documentation never referenced)
- **Source Type Distribution** (university vs blog vs API docs)
- **Query Classification** (how-to vs troubleshooting vs conceptual)

**Charts**:
1. **Query Frequency** - Bar chart of top 20 queries
2. **Source Distribution** - Pie chart of citation by source type
3. **Citation Heatmap** - Which sources are cited together

**Use Cases**:
- Understand what users ask about most
- Identify underutilized content
- Optimize documentation structure
- Plan content creation priorities

**Query Classification**:
```typescript
Intent Types:
- "how-to": Step-by-step instructions (e.g., "How do I create a collection?")
- "troubleshooting": Problem-solving (e.g., "Why isn't my form working?")
- "conceptual": Understanding concepts (e.g., "What is a CMS collection?")
- "reference": API/spec lookups (e.g., "Collection API parameters")
```

---

### 7. Trends ğŸ“ˆ

**Purpose**: Visualize temporal patterns and geographic distribution.

**Key Metrics**:
- **Queries Over Time** (daily, weekly, monthly)
- **Topic Trends** (rising, falling, stable)
- **Query Volume Heatmap** (by hour/day)
- **Geographic Distribution** (by country/region)

**Charts**:
1. **Time Series** - Line chart of query volume over time
2. **Heatmap** - Query volume by hour of day and day of week
3. **Geographic Map** - Choropleth map of queries by location
4. **Seasonal Patterns** - Overlay showing seasonal variations

**Use Cases**:
- Identify peak usage times
- Detect seasonal patterns
- Plan capacity for high-traffic periods
- Understand global user distribution

**Insights**:
- **Peak Hours**: Typically 9am-5pm PST (business hours)
- **Seasonal Trends**: Higher usage during product launches
- **Geographic**: 60% US, 25% Europe, 15% Rest of World

---

### 8. Topic Trends ğŸ”¥

**Purpose**: Detect emerging topics and user interest shifts.

**Key Metrics**:
- **Trending Topics** (rising, falling, stable)
- **Seasonal Patterns** (topic interest over time)
- **Emerging Topics** (new topics not seen before)
- **Intent Distribution** (how-to vs troubleshooting vs conceptual)
- **Word Cloud Data** (most common terms in queries)

**Charts**:
1. **Topic Velocity** - Scatter plot of growth rate vs query volume
2. **Topic Timeline** - Line chart showing topic interest over time
3. **Word Cloud** - Visual representation of query terms
4. **Intent Breakdown** - Donut chart of query intents

**Use Cases**:
- Identify emerging user needs
- Predict future content needs
- Detect product feature launches (spike in related queries)
- Guide documentation roadmap

**Topic Detection Algorithm**:
```typescript
1. Extract keywords from queries (NLP)
2. Cluster related queries by semantic similarity
3. Calculate growth rate: (recent_count / historical_avg) - 1
4. Classify trend status:
   - Rising: growth_rate > 0.2
   - Falling: growth_rate < -0.2
   - Stable: -0.2 <= growth_rate <= 0.2
```

---

### 9. Sentiment ğŸ˜ŠğŸ˜ğŸ˜

**Purpose**: Understand user feedback and identify improvement areas.

**Key Metrics**:
- **Sentiment Overview** (positive / neutral / negative %)
- **Complaint Categories** (incomplete answer, wrong info, etc.)
- **Improvement Trends** (sentiment over time)
- **Action Items** (prioritized fixes based on feedback)

**Charts**:
1. **Sentiment Distribution** - Pie chart of positive/neutral/negative
2. **Complaint Breakdown** - Bar chart of issue categories
3. **Sentiment Trend** - Line chart showing satisfaction over time
4. **Word Cloud** - Common words in negative feedback

**Use Cases**:
- Prioritize quality improvements
- Identify systemic issues
- Track sentiment after product changes
- Generate action items for engineering

**Sentiment Classification**:
```typescript
Positive:
- Contains: "helpful", "great", "exactly what I needed"
- Thumbs up without issue report
- Confidence > 0.7 and no regeneration

Neutral:
- No feedback provided
- Mixed feedback (both positive and negative)

Negative:
- Contains: "wrong", "incomplete", "confusing"
- Thumbs down
- Regenerated multiple times
```

**Common Complaint Categories**:
1. **Incomplete Answer** (35%) - Missing key details
2. **Wrong Information** (20%) - Factual errors
3. **Too Technical** (15%) - Explanation too complex
4. **Too Simple** (10%) - Lacks depth
5. **Missing Sources** (10%) - No citations provided
6. **Off Topic** (10%) - Answer doesn't address question

---

## Key Metrics Explained

### Confidence Score

**Definition**: Cosine similarity between query embedding and top Pinecone result (0-1 scale)

**Interpretation**:
- **High (> 0.7)**: Very relevant match, likely accurate answer
- **Medium (0.5-0.7)**: Moderately relevant, answer may need verification
- **Limited (< 0.5)**: Poor match, answer likely incomplete or off-topic

**Factors Affecting Confidence**:
- Quality of source documentation
- Specificity of user query
- Coverage of knowledge base
- Embedding model quality

---

### Active Users (DAU/WAU/MAU)

**Definitions**:
- **DAU**: Daily Active Users - unique users who made at least 1 query today
- **WAU**: Weekly Active Users - unique users who made at least 1 query in last 7 days
- **MAU**: Monthly Active Users - unique users who made at least 1 query in last 30 days

**User Identification**:
- Based on hashed IP address (no cookies/auth yet)
- Privacy-preserving (irreversible hash)
- Limitations: Shared IPs may inflate counts, VPN/proxy users may appear as multiple users

**Healthy Ratios**:
```
DAU/MAU = 0.2-0.3 (20-30% of monthly users are daily users)
WAU/MAU = 0.5-0.7 (50-70% of monthly users are weekly users)
```

---

### Cache Hit Rate

**Definition**: Percentage of queries served from KV cache vs. requiring fresh API calls

**Calculation**:
```typescript
Cache Hit Rate = (
  Cache Hits / (Cache Hits + Cache Misses)
)
```

**Impact**:
- **50% hit rate** = 50% cost savings on embeddings
- **10ms faster** response time (no OpenAI embedding call)

**Optimization**:
- Cache embeddings for 24 hours (good balance of freshness and savings)
- Normalize queries (lowercase, trim whitespace) before hashing
- Use query hash as cache key

---

### Cost per Query

**Definition**: Total API costs divided by total queries

**Calculation**:
```typescript
Cost per Query = (
  OpenAI Embedding Cost +
  OpenAI Completion Cost +
  Pinecone Cost +
  Cloudflare Cost
) / Total Queries
```

**Typical Range**: $0.01 - $0.03 per query

**Breakdown Example** (for 1000 queries):
```
OpenAI Embeddings:   $0.50  (50% cache hit)
OpenAI Completions:  $8.00  (avg 300 tokens output)
Pinecone:            $0.00  (free tier)
Cloudflare KV:       $0.20  (2000 reads)
Cloudflare D1:       $0.00  (free tier)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total:               $8.70
Cost per Query:      $0.0087
```

---

### Regeneration Rate

**Definition**: Percentage of queries where user clicked "Regenerate" button

**Calculation**:
```typescript
Regeneration Rate = (
  Queries with Regeneration / Total Queries
)
```

**Interpretation**:
- **< 10%**: Excellent - users satisfied with initial answers
- **10-20%**: Good - minor improvements needed
- **20-30%**: Fair - significant quality issues
- **> 30%**: Poor - major quality problems

**Common Regeneration Reasons**:
1. Low confidence / incomplete answer
2. User wants simpler explanation
3. User wants more technical depth
4. User wants more sources

---

## API Endpoints

All admin endpoints are located under `/api/admin/`:

### Content Gaps

```typescript
GET /api/admin/content-gaps

Response: ContentGapResponse {
  unanswered_questions: Array<{
    query: string;
    count: number;
    avg_confidence: number;
    last_asked: string;
    priority: "high" | "medium" | "low";
  }>;
  zero_citation_queries: Array<...>;
  most_regenerated: Array<...>;
  topic_clusters: Array<{
    topic: string;
    queries: string[];
    avg_confidence: number;
  }>;
  trending_gaps: Array<...>;
}
```

---

### Analytics Suite

All analytics endpoints follow this pattern:

```typescript
GET /api/admin/analytics/{category}

Categories:
- overview: UsageOverviewResponse
- performance: UsagePerformanceResponse
- quality: UsageQualityResponse
- cost: UsageCostResponse
- content: UsageContentResponse
- trends: UsageTrendsResponse
```

**Common Response Fields**:
- `timestamp`: ISO string of data snapshot
- `period`: Time range (e.g., "last_30_days")
- `generated_at`: Server time when response was created

---

### Topic Trends

```typescript
GET /api/admin/topic-trends

Response: TopicTrendAnalysisResponse {
  trending_topics: Array<{
    topic: string;
    status: "rising" | "falling" | "stable";
    growth_rate: number;
    query_count: number;
  }>;
  seasonal_patterns: Array<...>;
  emerging_topics: Array<...>;
  intent_distribution: {
    "how-to": number;
    troubleshooting: number;
    conceptual: number;
    reference: number;
  };
  word_cloud_data: Array<{
    word: string;
    frequency: number;
  }>;
}
```

---

### Sentiment Analysis

```typescript
GET /api/admin/sentiment

Response: FeedbackSentimentResponse {
  sentiment_overview: {
    positive: number;  // percentage
    neutral: number;
    negative: number;
  };
  complaint_categories: Array<{
    category: string;
    count: number;
    percentage: number;
  }>;
  improvement_trends: Array<{
    date: string;
    positive_rate: number;
  }>;
  action_items: Array<{
    issue: string;
    priority: "high" | "medium" | "low";
    affected_queries: number;
    suggested_fix: string;
  }>;
}
```

---

## Component Architecture

### Admin Components (`apps/web/src/components/admin/`)

#### AdminLayout

**Purpose**: Collapsible sidebar layout with responsive navigation

**Props**:
```typescript
interface AdminLayoutProps {
  children: React.ReactNode;
  activeTab: DashboardTab;
  onTabChange: (tab: DashboardTab) => void;
}
```

**Features**:
- Collapsible sidebar (desktop/tablet)
- Mobile hamburger menu
- Active tab highlighting
- Smooth transitions

---

#### MetricCard

**Purpose**: Display KPI metrics with icons and styling

**Props**:
```typescript
interface MetricCardProps {
  title: string;
  value: string | number;
  icon: React.ReactNode;
  trend?: {
    value: number;
    direction: "up" | "down" | "neutral";
  };
  subtitle?: string;
  variant?: "default" | "success" | "warning" | "danger";
}
```

**Usage**:
```tsx
<MetricCard
  title="Total Queries"
  value={1523}
  icon={<Hash className="w-5 h-5" />}
  trend={{ value: 12.5, direction: "up" }}
  subtitle="Last 7 days"
/>
```

---

#### Badge

**Purpose**: Status/priority indicators

**Props**:
```typescript
interface BadgeProps {
  variant: "success" | "warning" | "danger" | "info" | "default";
  children: React.ReactNode;
  size?: "sm" | "md" | "lg";
}
```

---

#### DataTable

**Purpose**: Sortable, paginated data tables

**Props**:
```typescript
interface DataTableProps<T> {
  data: T[];
  columns: Array<{
    key: string;
    label: string;
    sortable?: boolean;
    render?: (row: T) => React.ReactNode;
  }>;
  onSort?: (key: string, direction: "asc" | "desc") => void;
  pagination?: {
    page: number;
    pageSize: number;
    total: number;
    onPageChange: (page: number) => void;
  };
}
```

---

#### ChartContainer

**Purpose**: Wrapper for Recharts with consistent styling

**Props**:
```typescript
interface ChartContainerProps {
  title: string;
  subtitle?: string;
  children: React.ReactNode;
  actions?: React.ReactNode;
  height?: number;
}
```

---

#### LoadingState / EmptyState

**Purpose**: Loading skeletons and no-data states

**Usage**:
```tsx
{loading ? (
  <LoadingState count={3} />
) : data.length === 0 ? (
  <EmptyState
    icon={<Database />}
    title="No data available"
    description="Run some queries to see analytics"
  />
) : (
  <DataTable data={data} columns={columns} />
)}
```

---

## Development Fallback Architecture

### Overview

All admin API routes implement a **development fallback pattern** that provides mock data when the database is unavailable, while using real database queries in production. This ensures a smooth development experience without sacrificing production data integrity.

### Pattern Implementation

Every admin API route follows this structure:

```typescript
export async function GET(req: NextRequest) {
  try {
    const env = (process.env as unknown) as Env;

    // Development fallback: return mock data if DB not available
    if (!env.DB) {
      console.log('[DEV MODE] Returning mock data');
      return Response.json(getMockData());
    }

    // Production: use real database
    const db = getDatabase(env);

    // Real database queries...
    const data = await queryAll(db, 'SELECT ...');

    return Response.json(transformedData);
  } catch (error) {
    console.error('Error:', error);
    return Response.json({ error: 'Internal error' }, { status: 500 });
  }
}
```

### When Each Data Source is Used

| Environment | Database Available | Data Source | Use Case |
|-------------|-------------------|-------------|----------|
| **Local (no Wrangler)** | âŒ No | Mock Data | UI development, component testing |
| **Local (with Wrangler)** | âœ… Yes | Real Database | Full-stack development, API testing |
| **Production (Webflow Cloud)** | âœ… Yes | Real Database | Live analytics, production monitoring |

### Benefits

1. **Rapid UI Development**: Designers can work on UI without setting up infrastructure
2. **Consistent API Shape**: Mock data matches real data structure exactly
3. **Type Safety**: Both mock and real data use same TypeScript interfaces
4. **No Code Changes**: Same code works in dev and production
5. **Graceful Degradation**: Falls back to mock if database unavailable

### Real Database Implementation

All 9 admin routes have **complete real database implementations**:

#### 1. Content Gaps (`/api/admin/content-gaps`)
```typescript
// Real queries for unanswered questions
const unansweredQueriesRaw = await queryAll<QueryWithMetadata>(
  db,
  `
  SELECT DISTINCT
    q.query_text,
    rm.confidence_score,
    q.created_at,
    f.helpful
  FROM queries q
  LEFT JOIN responses r ON r.query_id = q.id
  LEFT JOIN response_metadata rm ON rm.response_id = r.id
  LEFT JOIN feedback f ON f.response_id = r.id
  WHERE rm.confidence_score < 0.3
  ORDER BY q.created_at DESC
  `
);
```

#### 2. Overview Analytics (`/api/admin/analytics/overview`)
```typescript
// Real DAU/WAU/MAU calculations
const dau = await queryFirst<{ count: number }>(
  db,
  `SELECT COUNT(DISTINCT user_ip) as count
   FROM queries
   WHERE created_at >= ?`,
  [sqlToday]
);

const wau = await queryFirst<{ count: number }>(
  db,
  `SELECT COUNT(DISTINCT user_ip) as count
   FROM queries
   WHERE created_at >= ?`,
  [sql7d]
);
```

#### 3. Performance Metrics (`/api/admin/analytics/performance`)
```typescript
// Real latency percentile calculations
const latencyData = await queryAll<{ latency_ms: number }>(
  db,
  `
  SELECT latency_ms
  FROM responses
  WHERE created_at >= ?
    AND latency_ms IS NOT NULL
  ORDER BY latency_ms ASC
  `,
  [sql30d]
);

const latencies = latencyData.map(r => r.latency_ms);
const percentiles = calculatePercentiles(latencies);
// Returns { p50, p95, p99 }
```

#### 4. Quality Metrics (`/api/admin/analytics/quality`)
```typescript
// Real confidence score aggregation
const avgConfidence = await queryFirst<{ avg: number }>(
  db,
  `
  SELECT AVG(confidence_score) as avg
  FROM response_metadata
  WHERE created_at >= ?
  `,
  [sql30d]
);

// Real feedback ratio calculation
const feedbackStats = await queryFirst<{ helpful: number; total: number }>(
  db,
  `
  SELECT
    SUM(CASE WHEN helpful = 1 THEN 1 ELSE 0 END) as helpful,
    COUNT(*) as total
  FROM feedback
  WHERE created_at >= ?
  `,
  [sql30d]
);
```

#### 5. Cost Analysis (`/api/admin/analytics/cost`)
```typescript
// Real query/response counts for cost calculation
const totalQueries = await queryFirst<{ count: number }>(
  db,
  `SELECT COUNT(*) as count FROM queries WHERE created_at >= ?`,
  [sql30d]
);

const totalResponses = await queryFirst<{ count: number }>(
  db,
  `SELECT COUNT(*) as count FROM responses WHERE created_at >= ?`,
  [sql30d]
);

// Calculate costs based on actual usage
const embeddingCost = (totalQueries.count * avgTokensPerQuery * EMBEDDING_COST_PER_TOKEN);
const completionCost = (totalResponses.count * avgTokensPerResponse * COMPLETION_COST_PER_TOKEN);
```

### Query Performance Optimizations

All routes implement these optimizations:

1. **Indexed Queries**: Use indexes on `created_at`, `user_ip`, `response_id`
2. **Parameterized Queries**: Prevent SQL injection with bound parameters
3. **Efficient Joins**: LEFT JOIN only when needed, avoid N+1 queries
4. **Date Range Filtering**: Filter early in WHERE clause to reduce rows scanned
5. **Aggregation at Database**: Use SQL SUM/AVG/COUNT instead of application-level

### Mock Data System

Mock data is generated in `/lib/dev-data.ts`:

```typescript
export function getMockContentGaps(): ContentGapResponse {
  return {
    unansweredQuestions: [
      {
        query: "How do I implement custom authentication?",
        count: 12,
        avg_confidence: 0.23,
        last_asked: new Date(Date.now() - 2 * 24 * 60 * 60 * 1000).toISOString(),
        priority: "high"
      },
      // ... more mock data
    ],
    // ... other fields
  };
}
```

**Mock Data Characteristics**:
- Matches production data structure exactly
- Uses realistic values and distributions
- Includes edge cases (empty arrays, null values)
- Updates timestamps relative to current date
- Provides enough variety for UI testing

### How to Test Both Modes

#### Test with Mock Data (No Database)
```bash
# Start Next.js without Wrangler
cd apps/web
pnpm dev

# Visit http://localhost:3000/admin
# All routes will return mock data
```

#### Test with Real Database (Local)
```bash
# Start with Wrangler (provides D1 binding)
cd apps/web
npx wrangler dev --local

# Visit http://localhost:3000/admin
# All routes will use local D1 database
```

#### Production (Webflow Cloud)
```bash
# Deploy to Webflow Cloud
webflow cloud deploy

# Visit https://your-app.webflow.io/admin
# All routes will use production D1 database
```

### Debugging Data Source

Check the browser console or server logs:

```typescript
// Mock data mode (development)
[DEV MODE] Returning mock content gaps data

// Real database mode (production)
[ADMIN] Content gaps query completed in 234ms
```

### Migration Path

If you need to remove mock data in production:

```typescript
// Option 1: Throw error if DB not available
if (!env.DB) {
  throw new Error('Database not configured');
}

// Option 2: Return empty data structure
if (!env.DB) {
  return Response.json({
    unansweredQuestions: [],
    zeroCitationQueries: [],
    // ... empty fields
  });
}

// Option 3: Keep current fallback (recommended for development)
if (!env.DB) {
  console.log('[DEV MODE] Returning mock data');
  return Response.json(getMockData());
}
```

### Best Practices

1. **Always Match Types**: Mock data must match `@shared` types exactly
2. **Test Both Modes**: Verify UI works with both mock and real data
3. **Keep Mock Data Fresh**: Update when adding new fields
4. **Log Data Source**: Always log which data source is used
5. **Handle Empty State**: Real data might have zero results, test this case

### Future Enhancements

**Planned for v1.6.0**:
- [ ] **Caching Layer**: Add 5-minute cache for expensive admin queries
- [ ] **Query Performance Metrics**: Track slow queries automatically
- [ ] **Data Validation**: Validate real data matches expected schema
- [ ] **Mock Data Generator**: CLI tool to generate mock data from production

---

## Data Export

### CSV Export

Each dashboard tab supports CSV export for external analysis.

**Trigger**: Click "Export CSV" button in top-right of dashboard

**Export Includes**:
- All visible data in current tab
- Timestamp of export
- Filter/date range applied

**File Format**:
```csv
# Webflow RAG Analytics Export
# Dashboard: Content Gaps
# Generated: 2025-11-19T10:30:00Z

Query,Count,Avg Confidence,Last Asked,Priority
"How do I create a collection?",23,0.42,"2025-11-19T09:15:00Z",High
...
```

**Implementation**:
```typescript
import { exportToCSV } from '@/lib/csv-export';

const handleExport = () => {
  exportToCSV(
    data,
    ['query', 'count', 'avg_confidence', 'last_asked'],
    'content-gaps-export.csv'
  );
};
```

---

## Best Practices

### Performance

1. **Lazy Load Tabs**: Only fetch data for active tab
2. **Cache Responses**: Use SWR or React Query for client-side caching
3. **Paginate Tables**: Limit rows to 20-50 per page
4. **Debounce Filters**: Wait 300ms before re-fetching on filter change

### Data Freshness

1. **Refresh Interval**: Auto-refresh every 60 seconds for Overview tab
2. **Manual Refresh**: Provide refresh button for all tabs
3. **Last Updated**: Display timestamp on all metrics

### Error Handling

1. **Graceful Degradation**: Show cached/stale data if API fails
2. **Error Messages**: User-friendly error messages (not stack traces)
3. **Retry Logic**: Exponential backoff for failed requests

### Accessibility

1. **Keyboard Navigation**: All tabs and actions keyboard-accessible
2. **ARIA Labels**: Descriptive labels for charts and tables
3. **Color Blindness**: Use patterns/icons in addition to color
4. **Screen Readers**: Announce data updates

---

## Troubleshooting

### Dashboard Not Loading

**Symptoms**: Blank screen or infinite loading spinner

**Solutions**:
1. Check browser console for errors
2. Verify API routes are running (`/api/admin/content-gaps` should return JSON)
3. Clear browser cache and localStorage
4. Check network tab for failed requests

### No Data Showing

**Symptoms**: "No data available" messages

**Solutions**:
1. Verify D1 database has data (`SELECT COUNT(*) FROM queries`)
2. Check that queries table has recent entries
3. Ensure API routes are using correct database binding
4. Verify mock data is being returned in development mode

### Charts Not Rendering

**Symptoms**: Empty chart containers or errors

**Solutions**:
1. Check that Recharts is installed (`pnpm list recharts`)
2. Verify data format matches chart requirements
3. Check browser console for Recharts errors
4. Ensure chart container has non-zero height

### Slow Performance

**Symptoms**: Dashboard takes > 5 seconds to load

**Solutions**:
1. Optimize database queries (add indexes)
2. Implement pagination for large datasets
3. Cache API responses on client
4. Reduce chart data points (aggregate by day instead of hour)

---

## Future Enhancements

### v1.6.0 (Planned)

- [ ] **Authentication** - Admin login with email/password
- [ ] **Role-Based Access** - Viewer vs Editor vs Admin roles
- [ ] **Alert System** - Email/Slack alerts for critical metrics
- [ ] **Custom Dashboards** - Create saved dashboard views
- [ ] **Report Scheduling** - Weekly/monthly email reports

### v1.7.0 (Planned)

- [ ] **A/B Testing** - Compare model/prompt performance
- [ ] **Query Recommendations** - Suggest related queries to users
- [ ] **Anomaly Detection** - Auto-detect unusual patterns
- [ ] **Real-time WebSocket** - Live metric updates without polling

### v1.8.0 (Planned)

- [ ] **Multi-Tenant** - Support multiple knowledge bases
- [ ] **Advanced Filtering** - Filter by date range, source type, confidence
- [ ] **Comparative Analysis** - Week-over-week, month-over-month comparisons
- [ ] **PDF Export** - Generate PDF reports from dashboard

---

## Support

**Documentation**: [CLAUDE.md](./CLAUDE.md) - Main project reference
**Issues**: Create an issue in your project repository
**Contact**: admin@your-domain.com

---

**Last Updated**: 2025-11-19
**Version**: 1.5.0
**Authors**: Webflow RAG Team
